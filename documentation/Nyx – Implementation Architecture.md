## Overview
Nyx is an autonomous AI coding agent designed to take high-level software development goals from a user and produce working software through planning, coding, testing, and iterative refinement. It is implemented in Node.js/TypeScript and follows a highly modular, layered architecture. The system is organized into clear components – a Command-Line Interface (CLI) for interactions, a Core Orchestrator for high-level control, a Planner for task breakdown, multiple specialized Agents for executing tasks, a suite of Tools for environment interactions, and Utility modules (for prompts, memory, etc.). This modular design separates concerns and makes the system extensible and maintainable. New tools or agent types can be added by implementing defined interfaces without altering core logic. The architecture emphasizes flexibility, allowing Nyx to **plan and execute** tasks, handle dependencies between tasks, run steps in parallel when possible, and make autonomous decisions (e.g. re-plan or spawn sub-agents) as needed during execution.

The system is distributed as an installable Node.js CLI package (e.g. via npm) so that developers can easily integrate it into their workflow. Once installed, Nyx can be invoked from the terminal using the `nyx` [command​](https://docs.nestjs.com/cli/overview#:~:text=Once%20installed%2C%20you%20can%20invoke,commands%20by%20entering%20the%20following).

Nyx also features **real-time observability dashboard** to Nyx’s interactive CLI interface. The terminal UI is split into two panes: a left pane for the standard command input and logs, and a new right pane showing live execution state (task plan, progress, agent status, metrics, etc.). This dashboard is built using a Node.js Terminal UI framework (such as **blessed**/**blessed-contrib**), enabling rich ASCII-based [visualizations​](https://www.npmjs.com/package/blessed-contrib/v/0.0.5#:~:text=blessed). The observability layer is integrated with Nyx’s core orchestrator and agents via an **observable state** mechanism (event emitters/reactive store) so that any internal state change is immediately reflected in the UI.

**Key Goals and Features:**
- **Layered Modular Architecture:** Distinct layers for CLI, Orchestrator, Planner, Agents, Tools, etc., each with a focused role.
- **Cohesive Code Organization:** Clear module boundaries and TypeScript interfaces for extensibility. (We will outline a directory structure and how to add new tools/agents easily.)
- **Task Planning with Dependencies:** Nyx interprets a user request and uses a planning module to produce a dependency-aware task graph, which can be executed sequentially or in parallel as appropriate.
- **Detailed Component Design:** Each component’s internal workings are specified – e.g. how prompts are constructed, how file locks and concurrency are handled, how the LLM context is managed, and how tools (shell, file system, browser) are abstracted.
- **End-to-End Flow:** From a user’s command to the final software output, Nyx autonomously goes through planning, task assignment, code generation, testing, debugging, and completion. It tracks tasks and iteratively repairs any issues (like failed tests) without user intervention.
- **Autonomous and Self-Directed:** The agent can make mid-execution decisions – for example, deciding to re-plan tasks, to fork off a sub-agent for a subtask, or to retry a different approach if a step fails – all within the architecture’s support.
- **Interactive Sessions:** The CLI supports multi-turn sessions, so Nyx maintains short-term memory of context across multiple user commands in one session (e.g. remember the project created in previous command when the user adds a new feature next).
- **Installable and executable via cli**: Once installed, Nyx can be invoked from the terminal using the `nyx` [command​](https://docs.nestjs.com/cli/overview#:~:text=Once%20installed%2C%20you%20can%20invoke,commands%20by%20entering%20the%20following).
- **Observability dashboard**: The Nyx terminal UI is split into two panes: a left pane for the standard command input and logs, and a new right pane showing live execution state
- **Built with Node.js and TypeScript:** Leverages the Node ecosystem for process control and I/O, with TypeScript for type-safe interfaces and maintainable code.
## Architecture and Components
 ([image]()) *Figure: High-level architecture of Nyx. Arrows denote the flow of information.* In this architecture, the **Core Orchestrator** is the central brain coordinating between the user interface, the planning module, various agents, and the environment via tools. The user interacts through the CLI, which sends the request to the orchestrator. The orchestrator invokes the **Planner** (often an LLM-driven component) to produce a **Task Dependency Graph** – a structured plan of what needs to be done. The orchestrator then manages the execution of these tasks by delegating them to appropriate **Agent** instances. Agents use available **Tools** (like shell commands, file operations, web access) to interact with the external environment (file system, OS, web) in order to carry out the tasks. The orchestrator maintains a **Session Memory/State** to remember context (like partial results, conversation history, and the current plan) and uses it to inform subsequent actions. Finally, once all tasks are completed, the orchestrator delivers the **Final Software/Results** back through the CLI for the user.

The architecture cleanly separates concerns: the Planner decides *what* to do, and the Agents handle *how* to do each part ([Plan-and-Execute Agents in Langchain - Comet](https://www.comet.com/site/blog/plan-and-execute-agents-in-langchain/#:~:text=The%20core%20of%20this%20agent,a%20planner%20and%20an%20executor)). This **plan-and-execute** design makes Nyx more reliable and scalable for complex objectives than a monolithic approach. Each component is described in detail below, followed by the execution flow that ties them together.
### Command-Line Interface (CLI) Layer
The CLI is the user-facing layer of Nyx. It provides an interactive shell where the user can issue high-level commands (goals or instructions for software to build or modify) and observe Nyx’s progress and results. Key aspects of the CLI layer include:

- **Interactive Session Management:** The CLI runs Nyx in an interactive REPL-like session. The user’s input (a command or request in natural language) is read and passed to the Core Orchestrator. The CLI keeps the session alive, allowing multiple back-and-forth interactions. Nyx’s short-term memory of recent interactions is maintained so that context persists within the session (e.g., it remembers the project state from previous commands).
- **Output Display:** The CLI prints out messages from Nyx, including plans, progress updates, and final results. This might include streaming partial outputs (for example, if Nyx is generating code, it can display intermediate steps or summaries).
- **Command Handling:** The CLI can parse certain meta-commands (like `exit` to quit, or maybe commands to save state) but primarily it forwards user requests to the orchestrator. It may also accept flags or arguments to configure Nyx (like verbosity level, or a project directory to use).
- **State Persistence:** In an interactive mode, the CLI keeps a reference to the Orchestrator (or a session state object) in memory so that subsequent commands use the same context. For example, if the user says “Add a login feature” after a previous “Create a web server project” command, the orchestrator will still have the project’s context loaded. Optionally, the CLI could support loading/saving session context to disk (so you can resume a session later), but by default it holds context in memory during the process’s lifetime.
- **Error Handling and Confirmation:** The CLI can capture unhandled exceptions or errors from deeper layers and present them to the user. If Nyx ever needs user confirmation (for example, executing a very destructive action), the CLI could prompt the user, though by design Nyx aims to be fully autonomous once a command is given.

*Implementation (TypeScript, Node):* The CLI might be implemented using Node’s built-in `readline` module or a library like **Inquirer.js** or **Commander** to handle input. The CLI module (e.g., `src/cli/`) could have a main file like `nyx.ts` that initializes the Orchestrator and handles the REPL loop. It would also handle Ctrl-C or exit events gracefully. The CLI prints outputs coming from the orchestrator (which may be streaming text). Short-term memory can simply live in the orchestrator’s state, persisted in a closure as the CLI loop continues.
### Core Orchestrator
The Core Orchestrator is the heart of Nyx’s logic. It coordinates the entire process from interpreting the user’s request to delivering the final outcome. The orchestrator’s responsibilities and design include:

- **Interpreting User Requests:** Upon receiving a request from the CLI, the orchestrator may perform some preprocessing (for example, normalizing the input or storing it as part of the session history). Typically, the orchestrator will treat the raw user request as input to the Planner (for task planning).
- **Invoking the Planner:** The orchestrator calls the Planner module to generate a structured plan (a set of tasks with dependencies) for the given request. This is often done by constructing a prompt for an LLM that describes the user’s goal and asks for a breakdown of steps. The orchestrator supplies any necessary context to the planner, such as what has already been built in the current session (so the plan can account for existing code if the user is asking for an incremental change).
- **Managing the Task Graph:** Once the Planner returns a set of tasks (the dependency graph), the orchestrator stores this plan in an internal **Task Graph** data structure. It keeps track of which tasks are pending, running, or completed. The orchestrator acts as an **execution engine** for this graph, scheduling tasks to run when their dependencies are satisfied. (We detail the task graph execution in a later section.)
- **Agent Coordination:** The orchestrator creates or assigns **Agents** to execute tasks. For each task that becomes ready (no unmet dependencies), the orchestrator either instantiates an appropriate agent or delegates it to an existing one. It may run tasks sequentially or in parallel, depending on the nature of the tasks and system capabilities. For example, if two tasks have no dependencies between them (they’re independent), Nyx can run them in parallel (concurrently) to speed up execution. The orchestrator ensures that concurrency is handled safely (e.g., two tasks writing to the same file will be serialized or otherwise managed to avoid conflicts via file locking, discussed below).
- **Tool Access and Mediation:** While agents are the ones “using” tools, the orchestrator often acts as the mediator between agent outputs and actual tool invocation. In one design, the orchestrator receives an agent’s intended action (like “I want to run tests now” or a command to write a file) and then calls the corresponding Tool module to perform it. This allows the orchestrator to log all actions, enforce any safety checks, and manage the results. For instance, if an agent decides to execute a shell command, the orchestrator’s tool interface will actually run the command and capture the output for the agent.
- **Session Memory and Context:** The orchestrator maintains the **session state**, including conversation history, current plan, and key artifacts (like a map of filenames to content written, or test results). It keeps a short-term memory of recent interactions and results, which it can feed into LLM prompts when needed to give context. For example, after finishing a task, the orchestrator might store a summary of the result (e.g., “Function X implemented successfully”) which the planner or agents can reference later. This short-term memory is essentially a transcript or state object that lives during the session ([AI Agents: AutoGPT architecture & breakdown | by George Sung | Medium](https://medium.com/@georgesung/ai-agents-autogpt-architecture-breakdown-ba37d60db944#:~:text=return%20the%20status%20of%20writing,1%2C%20the)). In the simplest form, it could be a list of the last N actions and outcomes (like a FIFO queue) ([AI Agents: AutoGPT architecture & breakdown | by George Sung | Medium](https://medium.com/@georgesung/ai-agents-autogpt-architecture-breakdown-ba37d60db944#:~:text=return%20the%20status%20of%20writing,1%2C%20the)), ensuring the LLM does not lose track of what has been done recently.
- **Autonomous Decision-Making:** The orchestrator monitors the progress and results of tasks and can take initiative when needed:
  - If a task fails (e.g., code fails to compile or a test fails), the orchestrator can decide to initiate a *repair loop*. It might create a new task (or modify an existing one) to fix the problem. For example, it can spawn a “Fix bug in module X” task and assign it to a code-generation agent, or re-invoke the planner to adjust the plan for the new situation.
  - If the whole plan seems to be going awry or an unexpected requirement emerges, the orchestrator can trigger a **re-plan**: calling the Planner again, possibly with additional context about what went wrong, to get a modified plan.
  - The orchestrator can also decide to **fork sub-agents** for parallel exploration. For instance, if one approach to solving a task isn’t working, the orchestrator could let one agent continue trying that approach while another agent is tasked with trying an alternative. The first one that succeeds could be adopted and the other aborted. (This is an advanced feature and requires careful resource management.)
  - At checkpoints, the orchestrator might have an agent perform a self-reflection or evaluation. For example, after completing all tasks, it might have a “QA Agent” verify the final output (running all tests or linting code) and decide to initiate fixes if any issues are detected.
- **Observability Integration:** _(New in this update)_ The Orchestrator is now instrumented to feed the **Observability Dashboard**. As it manages tasks and agents, it emits events or updates to a shared state that the dashboard listens to. Key points in the Orchestrator’s process (e.g. after the plan is created, when a task starts or completes, when an agent updates its status, etc.) trigger notifications. This ensures the right pane (dashboard) always shows the current state of the plan and progress. The Orchestrator might internally use Node’s `EventEmitter` or a similar mechanism to broadcast these state changes – for example, emitting a `"taskStarted"` event when a task begins, or updating a central observable store with the percentage of tasks completed. In Node’s event-driven architecture, objects (like the Orchestrator and agents) can emit named events that listeners (the dashboard components) respond to ​[nodejs.org](https://nodejs.org/api/events.html#:~:text=Much%20of%20the%20Node,to%20be%20called). This decouples the core logic from the UI – the Orchestrator doesn’t need to know how the UI works; it simply emits events or calls a dashboard update API, and the UI refreshes accordingly. (Under the hood, all Nyx components that emit events are likely subclasses of Node’s `EventEmitter` class, which allows multiple listeners to subscribe to events and react when `emit` is called ​[nodejs.org](https://nodejs.org/api/events.html#:~:text=Much%20of%20the%20Node,to%20be%20called).) The result is a clean integration where the Orchestrator’s core coordination logic remains unchanged except for these added hooks.
- **Finalization:** Once the plan’s tasks are all successfully executed (or the goal is otherwise achieved), the orchestrator compiles the results. This could mean ensuring all files are saved to disk, all tests pass, and the final software is ready. It then signals the CLI to output the final status to the user (e.g., “✅ Project completed successfully. All tests passed. The code is available in directory X.”). The orchestrator might also provide a summary of actions taken for the user’s understanding.

*Implementation:* In TypeScript, the orchestrator could be a class (e.g. `CoreOrchestrator`) with methods like `processUserRequest(request: string)`. It uses a `Planner` instance and has access to a registry of `Agent` types and `Tool` instances. Internally it holds structures for the task graph and session memory. The orchestrator likely runs an event loop or uses async/await logic to step through planning and execution. It may use Node’s event loop with promises to handle parallel tasks. For concurrency, it could maintain a set of active task promises; when one finishes, a callback updates the task graph and possibly starts new tasks. File locks or other shared resource protections would be enforced here (e.g., orchestrator checks if a file is in use by another task’s agent and queues the operation if so).
### Planner Module
The Planner is responsible for turning a high-level user request into a concrete sequence of tasks (the plan). Nyx’s Planner leverages the reasoning capabilities of an LLM to perform **task planning**. It creates a structured breakdown of the project or problem, identifying sub-tasks and their dependencies. This separation of planning from execution is a core design choice ([Plan-and-Execute Agents in Langchain - Comet](https://www.comet.com/site/blog/plan-and-execute-agents-in-langchain/#:~:text=The%20core%20of%20this%20agent,a%20planner%20and%20an%20executor)) that allows specialized focus on *what to do* before worrying about *how to do it*.

Key aspects of the Planner:
- **Input:** The Planner takes as input the user’s goal or command (e.g., “Build a CLI to manage a to-do list, with persistence to a JSON file”) and the current context/state. The context might include information like what has already been built (if this is a follow-up command in the same session) or any constraints (programming language to use, frameworks, etc., if provided by the user or known from context).
- **Prompt Construction:** The orchestrator constructs a prompt for the Planner LLM. This prompt typically contains:
  - A **system message** or instruction that defines the role of the LLM as a planner (for example: *“You are an expert software project planner. Break down the user’s request into step-by-step tasks with clear dependencies.”*).
  - The **user’s request** and any relevant context (like a summary of existing project state).
  - Guidelines or examples for output format – we want a structured output (e.g., JSON or a list format). For instance, the prompt may explicitly instruct: *“Output the plan as a JSON array of tasks, where each task has an `id`, a `description`, and an array of `depends_on` task IDs.”* Providing a few-shot example of a small plan in the correct format can significantly help the LLM produce the desired output ([LLM Powered Autonomous Agents | Lil'Log](https://lilianweng.github.io/posts/2023-06-23-agent/#:~:text=multiple%20tasks,do%20task%20parsing%20and%20planning)).
  - Optionally, a predefined list of allowed task types or categories to guide planning (e.g., “tasks can be of type `setup`, `code`, `test`, `debug`, `document`”), which helps the orchestrator later decide which agent to use for each task.
- **LLM-Based Planning:** The planner uses the LLM (e.g., GPT-4 via OpenAI API) to generate the plan. The LLM, guided by the prompt, will analyze the goal and break it into discrete steps. Each task is typically a short description of an action to perform (e.g., “Create a new Node.js project and initialize package.json”, “Implement the to-do list CLI interface”, “Write functions for adding and removing tasks”, “Write tests for the task functions”, “Run the tests”). The tasks include an indication of order or dependencies:
  - A simple approach is that the LLM outputs an ordered list, implying that later tasks depend on earlier ones, but this doesn’t capture parallelism explicitly.
  - A better approach (and what Nyx uses) is for each task to explicitly list which previous tasks it needs completed (dependencies). For example, a task object might look like:  
    ```json
    {
      "id": 3,
      "description": "Implement feature B",
      "depends_on": [2]
    }
    ```  
    meaning task 3 cannot start until task 2 is done. The LLM can be instructed to fill this out. Using an ID and dependency list allows for tasks that could be done in parallel (if they depend on different earlier tasks). In prior research, LLM planners have successfully output tasks with such structure when prompted with examples ([LLM Powered Autonomous Agents | Lil'Log](https://lilianweng.github.io/posts/2023-06-23-agent/#:~:text=multiple%20tasks,do%20task%20parsing%20and%20planning)).
- **Plan Validation and Parsing:** The raw output from the LLM (likely text in JSON or structured markdown format) is parsed by the Planner module (or orchestrator). We verify it’s well-formed (valid JSON, no missing dependencies, etc.). The Planner might include some post-processing: e.g., ensure there is a unique start task with no dependencies (or insert one if needed for an initial setup), sort tasks topologically, or handle any obvious mistakes (like circular dependencies) by adjusting or requesting a correction from the LLM.
- **Output:** The Planner returns a **Task Graph** object to the orchestrator. This could be a custom data structure or class, e.g., `TaskGraph` containing a list/array of Task objects and utility methods to navigate dependencies. Each `Task` would have properties like `id`, `description`, `dependencies` (list of ids), maybe a `type` or category, and status metadata (set to “pending” initially). If the LLM provided a task type (say, the task description or an explicit field indicates it’s a coding task vs a testing task), that is stored too.
- **Dashboard Representation:** The Planner’s output is the first thing displayed on the new dashboard’s right pane. As soon as the plan is ready, the Orchestrator emits an event to update the “Task execution plan” section of the UI. The plan can be visualized using a **tree widget** or similar ASCII graph representation. For instance, the root could be the overall objective, with child nodes for each top-level task, and further nested children for sub-tasks. Using the blessed-contrib library’s tree component, Nyx can easily render hierarchical tasks with branching lines connecting them ​[lightyears1998.github.io](https://lightyears1998.github.io/blessed-docs/widgets/blessed-contrib/tree/#:~:text=,Default%20%3A%20true). (The tree widget by default can draw line connectors in the console to represent parent-child relationships​[lightyears1998.github.io](https://lightyears1998.github.io/blessed-docs/widgets/blessed-contrib/tree/#:~:text=,Default%20%3A%20true), giving the user a clear overview of the task breakdown.) Each node in the tree can also show a status tag – e.g. `[ ]` for pending, `[✔]` for done, `[✖]` for failed – that the dashboard will update live as execution progresses.
- **Modularity:** The Planner is itself a module that could be swapped out. For instance, one could implement a non-LLM static planner for simple repetitive tasks or plug in a different planning algorithm. The orchestrator doesn’t need to know the details of how the plan is generated – only that it receives tasks and dependencies. The Planner module could reside in `src/planner/Planner.ts` with an interface like:  
  ```typescript
  interface Planner {
    plan(userRequest: string, context: PlanningContext): Promise<TaskGraph>;
  }
  ``` 
  where `PlanningContext` might include current project info, and `TaskGraph` is a structure of tasks.

*Example:* If the user requests “Create a simple calculator API with an addition and subtraction endpoint and write tests for it,” the Planner might output tasks such as (in natural language for illustration): 
1. **Set up project scaffolding** – depends on none.  
2. **Implement addition endpoint** – depends on Task 1.  
3. **Implement subtraction endpoint** – depends on Task 1.  
4. **Write tests for addition** – depends on Task 2.  
5. **Write tests for subtraction** – depends on Task 3.  
6. **Run all tests** – depends on Tasks 4 and 5 (and possibly on 2 & 3 if running tests requires the code implemented, which it does).  

This plan encodes that tasks 2 and 3 (the two features) can be done in parallel after setup, and test writing (4 and 5) can also proceed once their respective feature is done, etc. The orchestrator will use this graph to execute tasks in the right order (potentially concurrently). We’ll delve into the execution of the task graph next.
### Task Graph and Execution Engine
The output of the Planner is a **Task Dependency Graph** – a directed acyclic graph (DAG) where nodes are tasks and edges indicate dependencies (an edge from Task A to Task B means B depends on A’s completion). The Orchestrator uses this graph to drive the execution order, ensuring that at any point in time, only tasks whose prerequisites are finished will be executed. This approach allows maximum parallelism while respecting logical order.

**Task Representation:** Each task can be represented by a TypeScript interface or class, for example:  
```typescript
interface Task {
  id: number;
  description: string;
  type: string;              // e.g., 'setup', 'code', 'test', 'doc', 'misc'
  dependsOn: number[];       // ids of prerequisite tasks
  status: 'pending' | 'in_progress' | 'completed' | 'failed';
  result?: any;              // could store output or artifact reference
}
``` 
Tasks might also include fields like `assignedTo` (the agent handling it) or priority, but at minimum we need an ID and dependency list.

**Building the Graph:** After the Planner provides the tasks, the Orchestrator will typically:
- Build a mapping of task IDs to task objects for quick lookup.
- Compute an initial dependency count for each task (how many prerequisites it has). Tasks with no dependencies can be marked as ready to run.
- Potentially detect independent chains or group tasks by levels for clarity (though execution can be dynamic).

**Execution Algorithm:** The orchestrator runs a loop (or uses asynchronous events) to execute all tasks:
1. Identify all tasks that are **ready** to run (dependencies either zero or all dependencies already completed). Maintain these in a queue or list.
2. If multiple tasks are ready, Nyx can execute them in parallel. In Node.js, this could mean launching multiple asynchronous operations (e.g., multiple LLM calls or other actions at once). There might be a configurable limit to how many can run simultaneously to avoid overwhelming resources or hitting rate limits (for example, maybe not more than 2 LLM calls at once if using an API).
3. For each ready task, mark it as `in_progress` and dispatch it to an Agent (details on agent selection in the next section). This typically involves calling something like `agent.execute(task)`, which returns a Promise for that task’s completion (since the agent may do I/O and be asynchronous).
4. When a task completes successfully, mark it `completed` and capture any result (for example, if it was “write file X”, the result might simply confirm the file path or content written; if it was “run tests”, the result might include pass/fail info).
5. The orchestrator then finds any tasks that were waiting on this completed task, and decrements their dependency count or otherwise notes one more prerequisite done. If some of those now have all dependencies satisfied, they become ready. These are added to the ready queue.
6. If a task fails (an agent reports an error, or the outcome wasn’t as expected), mark it `failed`. The orchestrator at this point can decide to **branch** or adjust the plan:
   - It might retry the same task (maybe using a different strategy or after some fix).
   - It might create new remedial tasks (for example, if a test failed, create a “Debug and fix errors for feature X” task).
   - It could also halt the execution and report failure up to the user, but Nyx aims to be autonomous so it will prefer to handle the issue internally if possible. For instance, if “Run tests” failed, Nyx could spawn a debug agent to analyze test logs and then a fix agent to correct the code, effectively inserting new tasks into the graph dynamically.
   - If a new task is inserted, the graph is updated (with appropriate dependencies, likely the new fix-task depends on the tasks that created the code and the failing test, and the subsequent tasks depend on the fix-task now).
7. The loop continues until all tasks are either completed (success path) or no further progress can be made (in case of an unrecoverable failure).

This execution model naturally supports sequential and parallel execution: sequential chains occur for tasks linked by dependencies, and parallel execution happens when the graph has multiple independent branches.

 ([image]()) *Figure: Example Task Dependency Graph for a project.* In this example, Task **T2** (setup project structure) has no dependencies and will run first. Tasks **T3** and **T4** (implement Feature A and B) depend on T2 and can run in parallel after T2 completes. Task **T5** (integration) depends on both T3 and T4, so it waits for both to finish. Tasks **T6** and **T7** (write tests for A and B) depend on their respective features (T3 and T4) and could be done in parallel with T5. Finally, Task **T8** (run all tests) waits for integration and both sets of tests to be ready. Nyx’s orchestrator would use this graph to schedule tasks, often running T3 and T4 concurrently, then possibly T5, T6, T7 concurrently when possible, etc., to efficiently complete the objective. This graph-driven execution ensures correct ordering and maximizes concurrency.

**File Locking & Resource Management:** During execution, tasks (via agents) will perform actions like writing to files, reading files, or executing programs. To prevent conflicts:
- Nyx implements a simple **file locking** mechanism. For example, if Task 3 is writing to `featureA.js`, Nyx will lock that file (and perhaps the directory) so that if Task 5 or Task 4 tries to write to the same file, it must wait. In many cases, the plan will have been made to avoid two tasks writing the same thing, but it's possible they might both read or one write and another read concurrently. The orchestrator’s Tools (file system access) can check a lock table: if a file is locked for writing, another write or read can either queue or proceed in a controlled way. A simple approach is to not allow simultaneous writes at all, and allow reads in parallel with reads or writes (depending on consistency needs). Implementing locks can be done by an in-memory map of file path -> lock status and a promise queue.
- **Task Cancellation:** If Nyx decides to re-plan or spawn an alternative approach, it might cancel a running task’s agent. This is tricky with LLM calls (there’s usually no way to cancel a call once sent, other than ignoring its result). But for long-running code execution tasks or web browsing tasks, Nyx could implement a timeout or abort mechanism (e.g., using `AbortController` in Node for fetch requests, or killing a subprocess if running tests too long).
- **Progress Tracking:** The orchestrator logs progress as tasks begin and end. This can be used to provide live feedback to the CLI (e.g., “Working on task 3: Implement Feature A…”). It also aids in possibly saving state or resuming if needed.

**Visualization of Flow:** To represent the execution flow in the dashboard, Nyx can use a combination of textual representations. The “ASCII-style graph” mentioned is primarily the task dependency graph visualization. In addition, the dashboard might include a small **timeline or progress bar** indicating how many tasks have been completed out of the total. For example, a horizontal bar or a percentage counter can update as each task finishes. Blessed-contrib offers widgets like progress bars or donut charts that could be repurposed to show overall progress (e.g., a donut chart with a slice for completed vs remaining tasks). These elements provide at-a-glance feedback on execution flow alongside the task tree.
### Agents
Agents are the execution workhorses of Nyx. Each agent is responsible for carrying out one (or a group of related) task(s), typically by producing or modifying code, documentation, or other artifacts. In essence, an Agent takes a task description and *makes it happen*, using the LLM for reasoning or content generation and using Tools to effect changes in the environment.

**Agent Types and Roles:** Nyx’s design allows for multiple specialized agents, each focusing on a certain kind of task or operation. This specialization is not strictly necessary (one could design a single agent that can do everything based on context), but it provides a cleaner organization and the ability to optimize prompts for each role. Some potential agent types:
- **Coding Agent**: An agent responsible for writing or refactoring code. When assigned a task like “Implement the function to fetch weather data”, this agent formulates a prompt (including relevant context such as specifications or partial code) and calls the OpenAI API to generate code. It might use a model like GPT-4 with instructions to output only the code for the given function or file. The Coding Agent then takes the LLM’s response and integrates it into the project (e.g., writing to a file via the File Tool). It returns a status (success or error) and any relevant output (like file paths, code diffs) to the Orchestrator. The agent may also handle incremental development: if the task is large, it could generate code in pieces (though initial implementation likely one-shot per task).
- **Testing Agent**: This agent handles generating or running tests. If the task is “Write tests for module X” or “Run the test suite”, the Testing Agent might either use an LLM to generate test code or directly invoke the testing framework using Tools. For example, it could create a test file via the Coding Agent, then execute it using a shell Tool. The results (pass/fail, errors) are collected and reported.
- **Execution Agent**: Responsible for running code or shell commands. For tasks like “Execute the script to verify it works” or “Run the application”, the Orchestrator can call an Execution Agent. This agent might not need an LLM at all – instead, it uses Node’s `child_process` or similar to run commands. It captures output (stdout/stderr) and exit codes, sending those back to the Orchestrator. If an error occurs (non-zero exit or exception), the agent can mark the task failed and include the error message.
- **Analysis/Review Agent**: Optionally, Nyx might include an agent to review code for quality or analyze results. For example, after code is generated, a Review Agent (LLM-based) could check the code for potential issues or ensure it meets requirements. This agent could produce a report or suggestions, which the Orchestrator could use to spawn follow-up tasks (like “fix bug X identified by review”).
- **Documentation Agent**: Another possible agent is one that generates documentation or explanations for the produced code. If the user’s request includes documentation, this agent would use the LLM to create README content or docstrings.

In practice, these could all be instances of a generic `Agent` class with different prompt templates, or they could be separate classes inheriting from a base. For example, `CodeAgent extends Agent`, `TestAgent extends Agent`, etc., each overriding some specifics like how they format the prompt or handle results.

**Agent Interface:** In TypeScript, we can define a base Agent interface or abstract class, e.g.:
```typescript
interface Agent {
  name: string;
  execute(task: Task, context: AgentContext): Promise<AgentResult>;
}
``` 
Where `AgentContext` might include references to Tools or the orchestrator (so the agent can call back or request info), and `AgentResult` could include status and any output (like new file created, or test results). Each agent implementation would provide the `execute` method. The orchestrator doesn’t need to know the internal logic of the agent beyond calling `execute`.

**Prompt Construction in Agents:** Typically an agent, upon `execute`, will:
- Formulate a prompt for the LLM based on the task. This prompt likely includes:
  - A system message indicating the agent’s role (e.g., “You are a coding agent that writes code following best practices.”).
  - Context about the environment or codebase (for instance, a summary of relevant existing code if implementing a new feature in an existing file, or the content of a file to be updated).
  - The task description as the user prompt.
  - An instruction to produce the specific deliverable (e.g., “Output the complete code for the function, and nothing else.”).
  - If tools usage is allowed via the LLM, a description of available tools (though another approach is that the agent determines tool use itself; more on tool usage below).
- Call the LLM with this prompt (via a utility module that wraps the OpenAI API or another model API).
- Receive the LLM’s output, which could be:
  - Direct content (like code text, which the agent then writes to a file using a file tool).
  - Or a **Plan-of-Action** if we use a thought->action chain prompting style. For example, using a ReACT style prompt, the LLM might respond with something like “Thought: I should open the file to see current content. Action: read_file ‘path/to/file’” as a next step. The agent (or orchestrator) would then execute that tool action, get the result, feed it back to LLM in a next prompt, etc. This iterative tool-using process would continue until the LLM outputs the final result for the task. This is how agents can dynamically decide to use tools mid-task.
- Use Tools to effect changes: If the LLM outputs a final artifact (like code), the agent will use the File Tool to save it. If it outputs an action like “run tests”, the orchestrator/agent will invoke the Shell Tool to execute the test runner. The agent essentially translates between the LLM’s intentions and actual environment actions.

**Tool Access from Agents:** There are two main patterns for how an agent uses tools:
1. **By Orchestrator Mediation:** The agent’s LLM output is parsed by the orchestrator which then calls the tool. For example, AutoGPT-style, the LLM output might be a JSON with a field like `{"command": "write_file", "arguments": {"filename": "X", "content": "..."}}`. The orchestrator sees this and calls the File tool, then feeds the success/failure back to the agent’s LLM in the next prompt ([AI Agents: AutoGPT architecture & breakdown | by George Sung | Medium](https://medium.com/@georgesung/ai-agents-autogpt-architecture-breakdown-ba37d60db944#:~:text=3,command%20would)). This loop continues. The agent’s code (in TS) in this case is mostly orchestrating the LLM and delegating tool calls to the orchestrator.
2. **Via Function Calling (direct API support):** Modern LLM APIs (like OpenAI’s functions) allow us to register functions (tools) that the LLM can invoke directly. We can register the file write, file read, web search, etc., as functions in the prompt. The LLM then can return a function call payload which the agent code executes, then agent calls the LLM again with the function result. This is a cleaner version of the above and can be implemented in the LLM utility module. Nyx’s design is compatible with this approach – effectively it’s another way to parse and execute tool usage from the LLM.
3. **Internal Logic:** In some cases, an agent might use internal code instead of the LLM for certain operations. For example, a Testing Agent might not need the LLM to decide to run tests – the plan might directly have a “run tests” task and the agent just calls the test runner. Or a simple Documentation agent might have a template to fill. But generally, we rely on the LLM for consistency and creativity.

**Agent Autonomy and Collaboration:** Agents operate under the orchestrator’s supervision but also have some autonomy:
- An agent can decide it needs more information or to perform subtasks. If it’s using a ReACT prompt, it can “ask itself” to do things like read a file, search the web, etc. If an agent finds the task is too complex to do in one go, it might even invoke the Planner for a sub-plan (though usually we’d rely on the main Planner).
- Agents communicate their results or any new tasks back to the orchestrator. If an agent finishes and the result suggests a follow-up (like after writing code, perhaps it would be wise to run formatting or static analysis), the agent could either perform it if within scope or suggest it to the orchestrator to schedule as another task.
- In a multi-agent scenario, agents could work in parallel on different tasks (as the task graph permits). They might not directly talk to each other (all coordination goes via orchestrator and the shared environment), but effectively they collaborate by each fulfilling their part of the plan.

**Adding New Agents:** Thanks to modularity, introducing a new agent type is straightforward. You would:
- Create a new class implementing the `Agent` interface (e.g., `DocumentationAgent` for writing docs).
- Implement the `execute` method with its prompt and behavior.
- Register this agent type in a mapping (perhaps in the Orchestrator or a factory). For example, orchestrator might have a map like `{ "doc": DocumentationAgent }` mapping task types to agent classes.
- Update the Planner (if needed) to recognize when such tasks should be created (e.g., a “document” task type in the plan).

This way, no core logic needs changing – the orchestrator will simply instantiate the new agent for tasks of that type.

Agents also contribute to the **observability** in the new design. As they work on tasks, they send status updates back. Often this is done by simple return values or exceptions, but with the event-driven dashboard, agents can emit events too. For example, a Coding Agent could emit an event “codeGenerated” with the file name it created, or a Testing Agent could emit “testPassed” or “testFailed”. These events can be caught by the Orchestrator (for decision-making) and by the dashboard for real-time display. The **Agent Status** panel on the dashboard’s right side shows each active agent and what it’s doing. This could be implemented by having each agent register itself (or be registered by the Orchestrator) with the dashboard’s state. For instance, as soon as a task is assigned, the Orchestrator updates an “Agents Status” data structure like: `{ agent: "CodingAgent1", status: "Working on Task 3: Fetch API data" }`. If using an EventEmitter, the Orchestrator might emit `agentStatusChange` events whenever an agent starts or finishes a task, carrying info like agent name, task description, and new status (e.g., “idle” or “running”). The dashboard listens and updates the Agent list UI accordingly.
### Tools and Environment Interface
Tools are the mechanisms through which Nyx’s agents interact with the outside world. We abstract these interactions into a set of tools (also analogous to “commands” or “plugins”) that the agent can invoke. By designing a clean Tools interface, we make the system extensible (new capabilities can be added as new tools) and we keep the agent’s prompts and logic focused (the agent doesn’t have to know *how* a file is written or a test is run – it just triggers a tool to do it).

**Core Tool Types in Nyx:**
- **File Tool:** Enables reading from and writing to files in the project workspace. Likely subdivided into specific actions:
  - `read_file(path)`: Returns the contents of a file (or a portion if it’s large, maybe with some summarization if needed).
  - `write_file(path, content)`: Writes text to a file (creating or overwriting). The tool can handle directory creation if needed and returns a success or error message.
  - Possibly `append_file` or `find_in_file` if needed for more complex edits, though an agent could just read-modify-write.
  - The File Tool will implement file locking checks to avoid collisions (if two write operations occur, one will queue until the other finishes).
- **Shell/Process Tool:** Allows execution of shell commands or scripts. For example:
  - `run_command(commandString)`: executes a shell command (like running the project’s test suite, or a compiler, or installing dependencies with npm) and returns the stdout/stderr output.
  - This tool should be used carefully (to avoid destructive commands). Nyx might sandbox certain operations or require confirmation for very risky ones. However, running tests or simple commands is routine. The tool will capture the output (possibly truncating or summarizing if it’s huge) to feed back to the agent.
- **Browser/Web Tool:** Enables web access for the agent. This might include:
  - `search(query)`: perform a search (using an API like Bing or Google) and return results.
  - `open_url(url)`: fetch the content of a webpage (and maybe summarize it, or provide text content).
  - These are useful if the agent needs external information (for example, to look up documentation or error message solutions). However, use of such a tool should be guided by the agent’s prompt and the user’s allowances (some sessions might disable web access for security).
- **Operating System Tools:** We might have other environment-specific tools like:
  - `install_package(name)`: to install a Node package (though this could just be done via shell npm command).
  - `get_system_info`: to retrieve OS or environment info if needed.
  - `git_tool`: to perform version control operations if Nyx is integrated with a git workflow (like committing code, though this is optional).

**Tool Interface:** A generic Tool interface in TS could be:
```typescript
interface Tool {
  name: string;
  description: string;
  execute(args: any): Promise<ToolResult>;
}
```
Each concrete tool (FileTool, ShellTool, etc.) implements `execute`. However, tools often have multiple actions (sub-commands), so another design is to have each tool class with multiple methods, and identify actions by name. For example, a `FileTool` class with methods, or separate tools like `ReadFileTool`, `WriteFileTool`. The simplest approach used in prompts is often to list each action as a separate “command” name. So we might actually implement each as separate tool instances:
```typescript
class ReadFileTool implements Tool { ... }
class WriteFileTool implements Tool { ... }
class ShellTool implements Tool { ... }
```
and so on. The orchestrator can maintain a registry: `const tools: Tool[] = [ new ReadFileTool(), new WriteFileTool(), new ShellTool(), ... ]`. For each tool, the orchestrator provides the `name` and `description` to the LLM (in prompts) so that the agent knows what it can do ([AI Agents: AutoGPT architecture & breakdown | by George Sung | Medium](https://medium.com/@georgesung/ai-agents-autogpt-architecture-breakdown-ba37d60db944#:~:text=the%20ChatGPT%20API,json%20string%20returned%20by%20ChatGPT%E2%80%9D)). When the agent outputs a command (tool usage) by name, the orchestrator finds the matching Tool in the registry and calls `execute`.

**Adding New Tools:** To add a new capability, e.g., a `DatabaseQueryTool`:
- Implement a class for it with the required interface.
- Add an instance to the tools registry in orchestrator initialization.
- Update the prompt for the agent or planner if needed to inform the LLM of the new tool and how to use it.
No changes to agent or orchestrator logic are necessary since they work through the generic interface.

**Tool Execution and Safety:** 
- Tools run with the privileges of the user running Nyx. If Nyx is running on a developer’s machine, caution is needed. It’s wise to sandbox certain operations (for example, the Shell tool might disallow `rm -rf` or similarly dangerous commands unless explicitly allowed by the user). In a contained environment (like a cloud service), you’d enforce more restrictions.
- The output from tools is captured and often summarized. For instance, if running a test suite prints 500 lines, Nyx might trim it to the essential failure message or provide a summary to the agent, possibly with the option for the agent to request the full log if needed.
- The orchestrator ensures that tool outputs are fed back into the LLM context correctly. It might wrap the output in a system message like: “Output of last command:\n```\n<output here>\n```”. This way, the agent’s next LLM prompt includes the result of the action.

**Concurrency with Tools:** If multiple tasks use tools at once, Node’s event loop can handle multiple `execute` Promises concurrently. Care is taken e.g., by file locks for file tool and maybe a global limit on heavy operations. For example, two shell commands can run in parallel (Node can spawn two child processes), but we might limit CPU-intensive tasks to avoid overload.
### Extensibility and Modularity
Nyx was built with extensibility in mind. The architecture’s modular design allows developers to extend or customize the system in various ways:
- **Adding New Agents:** One can introduce a new agent by implementing the expected interface (e.g., a class with a `performTask(task, context)` method). The new agent can then be registered in the Orchestrator’s agent registry, possibly with a type or keyword so the Orchestrator knows which tasks it can handle. For instance, one could add a “OptimizationAgent” that specifically improves code performance if a task is labeled as an optimization task. Because agents are loosely coupled and communicate mainly through the Orchestrator (and events), new agents won’t break existing ones. The Orchestrator could also be configured to use a different Planner agent (if, say, a more advanced planning model becomes available) by swapping out the module.
- **Adding Tools:** Similarly, new tools can be added to expand Nyx’s capabilities. For example, adding a “Database Tool” might allow an agent to run SQL queries or seed a database as part of a setup task. Tools can be injected into agent contexts or accessed via a global tools registry. The design ensures that tools are abstracted; an agent asks for a resource (like “write file”) without needing to know the filesystem specifics, so replacing that underlying mechanism (maybe to target a remote filesystem or a virtual FS) is possible without agent changes.
- **Configurable Workflow:** The planning-execution loop can be configured or even bypassed. For simpler tasks, one might skip planning and directly invoke a specific agent (Nyx could support commands like `nyx run code.py` to just execute code with the Execution Agent). The CLI interface can be extended with subcommands or flags to control this. For instance, a `--no-plan` flag could instruct Nyx to not generate a multi-step plan and treat the entire user prompt as a single coding task.
- **Extensible CLI:** Because Nyx is a Node.js CLI app, we can use libraries to parse commands and options (like yargs or commander). The CLI could allow users to specify which OpenAI model to use, adjust verbosity, or enable/disable the dashboard. As an example, Nyx might support `nyx --dashboard=off "Your instruction"` for a minimal output mode, or `nyx --agent-config=config.json "..."` to load custom agent settings. The documentation would outline these, and the architecture would parse and pass relevant flags to the orchestrator/agents.
- **Modular Integrations:** Each part of Nyx (Planner, each Agent, Tools, Dashboard) is relatively independent. This also means Nyx could be used as a **library** if someone wanted to embed the planning/execution engine in another application. Because Nyx is published on npm, a developer could `require('nyx-agent')` in code and call an API to run a Nyx plan programmatically, though the primary usage is via CLI. The core logic (Orchestrator and agents) can run without the CLI too, which is how we integrate the TUI: the CLI layer sets up the dashboard and passes control to the Orchestrator. If the CLI were not used (e.g., in a headless automated script), the Orchestrator can run to completion using the same underlying logic (it would simply not emit UI events, or the events would not be hooked, if the dashboard is not initialized).

In summary, the architecture ensures that **previous features remain intact** and continue to function. The new observability functionality is built atop the existing event-driven design and does not fundamentally change how Nyx plans or executes tasks – it only observes and displays that internal process in real time. Developers can continue to rely on the Planner/Orchestrator/Agent system as documented originally, and now have improved visibility into the system’s operation.
### Session Memory and Context Management
Nyx’s ability to carry context across interactions and throughout a long task is crucial. The **session memory** refers to information retained during a single run of Nyx (especially in an interactive CLI session). This includes conversational context (the dialogue with the user and internal reasoning) and world state (the code and artifacts created so far, outcomes of tests, etc.).

Key points about memory management:

- **Short-Term Memory:** This is the recent history of the conversation and operations that is kept readily available for prompting the LLM. For example, the last N exchanges with the LLM (like thoughts, tool actions, results) and any important facts discovered are kept in a buffer. Nyx uses a rolling window strategy: as the conversation grows, older, less relevant details are dropped or summarized, ensuring the prompt stays within token limits. This is similar to AutoGPT’s approach where only the most recent interactions (e.g., the last 9 messages) were kept in the immediate context ([AI Agents: AutoGPT architecture & breakdown | by George Sung | Medium](https://medium.com/@georgesung/ai-agents-autogpt-architecture-breakdown-ba37d60db944#:~:text=return%20the%20status%20of%20writing,1%2C%20the)). The short-term memory typically includes:
  - The user’s original request and any follow-up instructions.
  - The tasks plan (maybe not verbatim the whole plan, but the current task being worked on can be provided to the agent).
  - The outcome of recently executed tasks (e.g., “Task 3 completed, feature A implemented successfully”).
  - If mid-task, the steps the agent has taken and results (for example, “Tried to run tests, 2 failures encountered with messages X and Y”).
- **Long-Term Memory (within session):** If the session is very lengthy or the project is large, Nyx could employ strategies to not forget earlier important details. For example, after completing a major section, Nyx might generate a summary of it and store that summary. If needed later, that summary can be recalled into prompt. In advanced setups, one could integrate a vector database: storing embeddings of content (code or conversation) to later retrieve relevant bits when a certain topic comes up ([AI Agents: AutoGPT architecture & breakdown | by George Sung | Medium](https://medium.com/@georgesung/ai-agents-autogpt-architecture-breakdown-ba37d60db944#:~:text=added%20to%20memory%207,pairs%2C%20we%20can%20use%20local)). However, for an initial implementation, this might be overkill. Simpler:
  - Keep an in-memory object of all files created/modified. If the agent needs to see a file, it can be loaded from disk (which is effectively a form of memory – the code itself is an external memory).
  - Keep a log of decisions and rationales such that if the user asks “why did you do X?” or the agent needs to remind itself, it can refer to that log.
- **Cross-Command Memory:** In interactive mode, if the user issues separate commands (like building in increments), Nyx should persist relevant state across those commands:
  - The simplest persistence is the file system – the code written in the first command naturally persists for the second.
  - Additionally, Nyx can keep a `SessionState` object in memory that includes a high-level summary of what’s been done (e.g., what features are implemented, what technologies are being used). The second command’s planner can use that context. For instance, if first command created a Python project, the second command asking for a new feature should be planned in Python, not suddenly switching languages or frameworks.
  - Some ephemeral memory like conversation history could be reset between top-level commands if desired (maybe treat each as a fresh planning phase), but any interactive clarification by the user within the same command sequence should be kept.
- **Prompts and Memory Use:** The orchestrator is in charge of constructing LLM prompts and will insert memory where appropriate. For example, when invoking the Planner, it might add: “Here is what has been done so far: ...” with a concise summary of the current project (from memory). When invoking an Agent to write code, it will provide the context of that code (possibly by reading the current file content or outline from memory).
- **Memory Limits and Cleanup:** Because LLM context windows are limited, Nyx uses strategies like:
  - Only include the *relevant* part of memory for a given operation. E.g., if working on feature A’s code, no need to include details of feature B’s implementation in the prompt.
  - Use summarization: compress lengthy logs into key points. Perhaps after a test run with multiple failures, summarize the failures rather than pasting all raw output.
  - Not storing raw code in memory when not needed – instead, fetch from files on demand.
  - If a session becomes too long and unwieldy, Nyx could suggest concluding and starting a fresh session with the current project saved.

- **No Long-Term Memory between sessions (by default):** Unless explicitly saved, once Nyx terminates, its memory is lost except for the artifacts on disk. A future enhancement could allow saving a session state (including embedding some knowledge of code to reload later). As a design note, this is a potential extension but not a requirement.
### End-to-End Execution Flow
To illustrate how all these components interact, let's walk through how Nyx processes a user command from start to finish, using the architecture described:

1. **User Input (CLI):** The user types a request in the Nyx CLI, for example: *"Create a simple Node.js REST API for managing tasks, including routes to create, list, update, delete tasks, and include unit tests."* The CLI passes this string to the Core Orchestrator’s `processUserRequest`.

2. **Planning Phase (Orchestrator + Planner):** The orchestrator calls the Planner with the user’s request. It provides any context (if it’s the first command in a new session, context might be minimal, maybe just “Use Node.js/TypeScript” if that’s a default or configured setting). The Planner’s LLM prompt is constructed with instructions to output a detailed plan in JSON. The Planner LLM analyzes the request and generates a plan, for example:
   - Task 1: **Set up project** (initialize npm, basic structure) – no dependencies.
   - Task 2: **Implement “create task” endpoint** – depends on 1.
   - Task 3: **Implement “list tasks” endpoint** – depends on 1.
   - Task 4: **Implement “update task” endpoint** – depends on 1.
   - Task 5: **Implement “delete task” endpoint** – depends on 1.
   - Task 6: **Write unit tests for task routes** – depends on 2,3,4,5 (all features implemented).
   - Task 7: **Run tests** – depends on 6.
   (The planner ensures all CRUD features and tests are represented, and that tests wait for features to be done. Many of those feature tasks (2–5) can proceed in parallel after task 1.)
   The orchestrator receives this plan structure (as a TaskGraph object). It might log or display the planned tasks to the user for transparency.

3. **Task Execution Loop Begins:** The orchestrator inspects the task graph. Task 1 (setup project) has no dependencies, so it’s ready. Tasks 2–5 depend on 1, so not yet. It can start Task 1 immediately.
   - The orchestrator selects an agent for Task 1. Task 1 might be categorized as a “setup” type. We might handle setup with a CodeAgent (since it involves creating files like package.json) or a specialized InitAgent. Suppose we use a CodeAgent.
   - The orchestrator calls `agent.execute(Task1)`. The agent forms a prompt: it knows the task is “set up project structure”. It might ask the LLM to generate a basic Node.js project initialization (maybe just a package.json and an index.ts file, possibly a README).
   - The LLM responds with content for these files or commands to run. For example, it might output: “Action: write_file `package.json` with content {...}, then Action: write_file `src/index.ts` with content ...”. The agent (with orchestrator) executes these: FileTool writes the files.
   - Task 1 completes (status to `completed`). Result could be a list of files created.

4. **Parallel Task Execution:** Now tasks 2,3,4,5 become unblocked (each depended only on 1). The orchestrator can dispatch multiple agents in parallel for these tasks:
   - It may create one agent per task, or reuse a single CodeAgent sequentially. To leverage parallelism, let’s assume multiple agents (or the same agent but invoked concurrently with separate prompts – in practice, separate instances or ensuring prompts don’t interfere).
   - For Task 2 “Implement create task endpoint”, orchestrator picks a CodeAgent, provides context (the project structure is set up; maybe an empty server exists or a framework to use is known). The agent prompt might include: “Implement the POST /tasks endpoint in the server. Currently, assume we have an Express app set up in `index.ts` (or the agent can create one if not). The endpoint should accept task data and store it. Possibly use an in-memory array or simple storage as persistence for now.” The agent might need to also decide on data structures; the LLM will produce the code for this endpoint and possibly modify or create files (like a controller or directly in `index.ts`).
   - At the same time, Task 3 “Implement list tasks” could be handled by another agent with a similar prompt specific to GET /tasks.
   - And similarly for update and delete (Tasks 4,5).
   - All these four tasks can proceed concurrently. They might all need to edit the server code. This is a potential conflict: if all try to edit the same file (e.g., `index.ts` with Express routes), we must handle it. Strategies:
     * Ideally, the planner could have broken the tasks in a file-specific way (like create separate route modules per endpoint). But if not, Nyx must coordinate. Perhaps each agent writes its code in a separate buffer, and after all are done, a merge step could integrate them. Simpler: do them sequentially if they target the same file. To be safe, orchestrator might limit these tasks to run one at a time if it detects they all depend on modifying the same file (this is an area where dynamic dependency could be introduced: tasks 3–5 might effectively depend on task 2 if they all modify the same region).
     * For demonstration, suppose the planner (or initial setup) created a basic Express app and left room for endpoints, and each task writes in different parts of code. The orchestrator can allow parallel but with file locks: when Task 2 agent goes to write `index.ts`, it locks it; if Task 3’s agent also needs `index.ts`, it will wait until unlocked. In effect, they end up sequential for that file access but they could do other reasoning in parallel.
   - Each of tasks 2–5 completes, writing code for each endpoint. At this point, the project’s codebase has all features implemented.

5. **Dependent Task Execution:** Once tasks 2–5 are done, Task 6 “Write unit tests” is unblocked (it depended on all feature tasks). The orchestrator now launches a TestAgent for Task 6.
   - The TestAgent’s prompt might be something like: “Generate a test suite for the task API. There should be tests for creating a task, listing tasks (perhaps ensure it lists what was created), updating (ensure the changes persist), and deleting (ensure deletion works). Use a testing framework like Jest. Output the test code in a file, e.g., `test/api.test.ts`.” 
   - The agent might use the FileTool to create `test/api.test.ts` with the generated tests. It may also require adding a dependency (like installing Jest) – which it can either do via a ShellTool call (e.g., running `npm install jest`) or by adding to package.json and relying on user to install. A fully autonomous agent would handle installation via the shell if needed.
   - Task 6 completes with tests written (and maybe dependencies installed).

6. **Final Task – Testing (Verification):** Task 7 “Run tests” is now ready. The orchestrator dispatches perhaps a simple agent or just directly uses the Shell Tool to execute the test command (since this task might be marked as a `execute` type).
   - The ShellTool runs `npm test` or similar. The output (test results) is captured.
   - Suppose some tests fail (very possible on first try). The Task 7 would then report failure. Orchestrator marks it as `failed` (or completed with errors) and triggers the autonomous problem-solving behavior:
     * The orchestrator spawns a DebugAgent (or could re-use the CodeAgent in debug mode) for a new task: “Fix failing tests”. This new task is added to the graph, depending on the completed tasks 2–6 and gating final completion. Or orchestrator could simply loop within Task 7 (some designs treat the test running and fixing as part of the same task via iterative agent steps).
     * The DebugAgent reads the test output (from the ShellTool result, which is in memory). The prompt might be: “Some tests failed. Here are the failing test cases and error messages: ... . Analyze the cause and fix the code accordingly. Provide the updated code for the relevant part.”
     * The LLM might identify, say, that the update endpoint didn’t handle the case of a non-existing task ID properly, causing a test failure. It then suggests a code change. The agent uses FileTool to apply this change (maybe it rewrites the function in index.ts).
     * After the fix, the orchestrator decides to re-run the tests (either as part of the debug agent’s routine or as a repeat of Task 7). Let’s say it runs them again via ShellTool.
     * If tests now pass, Task 7 succeeds on second attempt. If not, this loop (analyze -> fix -> test) can repeat. Nyx can have a limit on iterations to avoid infinite loops (and eventually prompt the user if stuck).
   - Finally, assume tests pass. All tasks are now completed successfully.

7. **Completion and Output:** The orchestrator gathers the final outcome. It might generate a brief report: “All endpoints implemented and tests are passing. Created files: src/index.ts, src/... , test/api.test.ts, etc.” It may also archive the project (or initialize a git repo commit if that was part of tasks). 
   - The CLI then presents the success message to the user. Possibly it could offer next steps like “You can run the server via `npm start`” or “Do you want me to run the server to try it out?” if interactive.
   - The session remains open for further commands. The user could then say, for example, “Great, now add an authentication feature to that API”. Nyx would take into account the existing code (persisted on disk and in session memory) and plan new tasks accordingly in a new cycle.

Throughout this process, Nyx made many autonomous decisions: how to break down the problem, how to implement each part, how to fix issues that arose, and it orchestrated multiple agents possibly in parallel. The design ensured that each part of the system had a clear responsibility, and the orchestrator managed the complexity by sticking to the plan (with adjustments when needed).
## Real-Time Observability Dashboard (TUI) – Design & Implementation
One of the major updates to Nyx is the introduction of a real-time **Observability Dashboard** in the CLI. This dashboard provides live insight into Nyx’s thought process and actions, making the tool more transparent and user-friendly. It is implemented as a Text-based User Interface (TUI) that augments the traditional console output. The TUI is rendered directly in the terminal (no GUI needed), using ASCII/ANSI characters to draw panels, charts, and text. Below we describe the design of the dashboard, the technical choices for implementation, and how it ties into Nyx’s runtime events:
### Dashboard Layout – Split Panel Interface
The terminal interface is divided into two primary panels side by side:
- **Left Panel – Command & Log Panel:** This panel occupies the left portion of the terminal window. It serves two purposes: it’s where the user enters input/commands, and it displays Nyx’s direct textual output (like a log). Essentially, this replicates and improves the original CLI experience. When Nyx is running, all status messages, confirmations, errors, and other textual feedback from the Orchestrator and agents appear here in a scrollable region. For example, lines like “Planner: 5 tasks generated” or “CodingAgent: Writing file `weather.py`” will be printed in the left panel in chronological order. This panel also includes an **input prompt** at the bottom (or a designated area for user typing). The user can type commands or responses here; for instance, the initial command to start Nyx (if not provided as an argument) can be entered, or responses to yes/no prompts, or even special commands to control Nyx (like typing “pause” to pause execution, if that feature exists). The UI framework will capture keystrokes in this panel and forward them to Nyx’s input handler. The left panel functions much like a terminal within a terminal – it could be implemented using a `blessed.textarea` or `blessed.log` element for output and a `blessed.textbox` for input. The output area will auto-scroll as new lines are added (blessed’s log widget supports this), ensuring the latest messages are visible.
- **Right Panel – Live Dashboard Panel:** The right portion of the screen is the new dynamic dashboard. It is subdivided into sections that display various aspects of Nyx’s internal state in real time. The content of this panel updates continuously (or event-driven) as the Orchestrator and agents progress through tasks. Key sections in this panel include:
    - **Task Plan & Progress:** A section listing the tasks from the Planner (the execution plan) and showing their status. This is often visualized as a tree or hierarchical list. Each entry might show a task name and an icon or color indicating status (pending, running, done, error). Completed tasks might turn green or be checked off, the current task might be highlighted, etc. This gives the user an immediate sense of where Nyx is in the overall plan. If tasks are structured hierarchically, this also implicitly shows the execution flow (e.g., tasks 1.1, 1.2 under a main task 1, etc.). An ASCII tree drawing, as mentioned earlier, is well-suited here. We can use the blessed-contrib **Tree** widget to render this structure​[lightyears1998.github.io](https://lightyears1998.github.io/blessed-docs/widgets/blessed-contrib/tree/#:~:text=,Default%20%3A%20true). The tree can initially be fully expanded to show all planned tasks, or perhaps collapsed to top-level tasks and expanded as they start (for clarity). The “progress” aspect can be supplemented by a numeric indicator (e.g., “Tasks completed: X/Y”) or a progress bar widget showing the fraction done. For example, blessed-contrib’s **progress bar** could show a bar that fills as tasks complete. In addition, a **percentage** or **ETA** might be displayed if meaningful (though ETA is hard to estimate for AI tasks).
	- **Agent Assignments & Status:** A section showing each Agent in the system and what it’s currently doing. This could be a simple list or table. For instance:
		- This tells the user which agents are active at any given moment. If multiple instances of an agent type exist (less likely in initial design), they can be listed separately (e.g., multiple CodingAgents if parallel tasks). The status could be "Idle", "Running TaskName", "Error" if an agent hit an issue, etc. This section is updated whenever an agent picks up or finishes a task. Internally, implementing this might involve each agent process emitting an event when its status changes, or the Orchestrator updating a shared "agentsStatus" object. The dashboard could use a **table widget** or just formatted text. Blessed-contrib has a **Table** that could neatly align columns (Agent Name, Status, Task working on). If using a table, the first column is agent name, second is status, third could be additional info (like error message snippet if failed). Alternatively, a simpler text list (blessed.list) can be used for easier updates.
    - **Live Statistics & Counters:** This area displays numeric metrics about the current run. These statistics update as the run progresses. Examples of stats to show:
        - **Tasks Completed:** how many tasks have finished successfully out of total tasks. (If tasks have sub-tasks, we might count leaf tasks or all tasks – likely all tasks in the plan for simplicity.) For example, “Tasks: 3/5 completed”.    
        - **Current Task:** the name or ID of the task currently being executed (or “Idle/Waiting” if none at the moment). If multiple tasks can run in parallel, it might list the ones in progress.
        - **Failures/Errors:** number of tasks that have failed (if any). If a failure occurs and is handled, this counter allows the user to notice it even if it was quickly resolved.
        - **LLM API Calls:** how many API calls to OpenAI (or other models) have been made so far during this session. This is a useful metric for users to track cost and performance. The Orchestrator can increment a counter each time an agent calls the OpenAI API.
        - **Elapsed Time:** time since the run started. A simple timer can be shown (minutes:seconds).
        - **Memory Usage:** (if relevant) how much memory the process is using or how large the prompt contexts are, etc. This is more of a debug metric but can be interesting in long runs. Node provides `process.memoryUsage()` which could be polled periodically to update this value.            
        - **Queued Actions:** if Nyx has any queued follow-up actions or waiting tasks (for example, tasks that became available but are queued behind others), this could be indicated.  
            These stats can be displayed in a textual form (perhaps a small multiline text box). Another approach is to use **gauges or charts** for some – e.g., a gauge showing memory usage as a fraction of some expected max, or a bar chart for # of API calls over time. Blessed-contrib includes widgets like **Gauge**, **Sparkline**, **Donut**, etc., which can add visual flair. For instance, a donut chart could represent the percentage of tasks done vs pending vs failed as differently colored segments updated live. For initial implementation, simple text is straightforward and less error-prone. The key is that whenever the underlying values change, the dashboard refreshes them. The Orchestrator’s event emitter might have something like `statsUpdate` events, or these can be recomputed on each render cycle.
    - **Execution Flow Visualization:** If possible, the dashboard can also include an ASCII diagram of the task execution flow. The primary way we’re visualizing flow is via the task tree (which inherently shows structure). However, an additional mini-visual could be included, such as a **flow chart** or pipeline view. This is more challenging in text, but one could imagine for a linear series of tasks, printing something like:
	    arduino
        CopyEdit
        `[Task 1: Plan] -> [Task 2: Coding] -> [Task 3: Testing] -> [Task 4: Complete]`
        with the current task highlighted. If tasks branch, this becomes complex to draw in one line. So this may be optional. We might use the tree for structure and rely on the progress bar for overall flow.  
        Another idea is to use an **ASCII Gantt chart** style: a timeline with tasks labeled on a time axis. This is likely too advanced for now and not explicitly requested, so we will skip it in this iteration.
    - **Dynamic Info / Debug Panel:** This is a catch-all area for any other information Nyx wants to share. It could show the content of the current prompt sent to the LLM (for advanced users interested in the prompt engineering), or the result of the last command executed. However, exposing the raw prompts might clutter the UI. Alternatively, this area can be used by Nyx to provide tips or next steps after completion. Since the question specifically mentioned “any dynamic information Nyx wants to share (e.g. queued actions, memory usage, LLM call stats)” – we’ve covered memory and LLM stats in the Stats section. Queued actions (if meaning tasks waiting to run) could also be listed, though if we show pending tasks in the task list, that’s handled. Perhaps by “dynamic info” the idea is just to allow printing arbitrary messages or data structures. The left panel is already logging most messages. So, we interpret this as ensuring the dashboard design is flexible to include new sub-widgets if needed for debugging or new features.

The dashboard’s layout can be implemented with the **blessed-contrib grid** layout system for ease. We can treat the whole screen as a grid, say 12 columns by 12 rows, and allocate regions: for example, columns 0-6 for the left panel (full height), and columns 6-11 for the right panel. Within the right panel region, subdivide into rows for each section: perhaps the top half for Task Plan/Progress (with tree view and maybe progress bar), the next quarter for Agent Status and Stats, and the bottom quarter for any additional info or logs. Blessed-contrib makes it straightforward to position widgets in a grid by specifying row/col spans​[github.com](https://github.com/yaronn/blessed-contrib#:~:text=GitHub%20github,%2F%2Fgrid.set). Alternatively, one can manually create a `blessed.box` for the left panel and one for the right panel, then within the right panel box, create sub-boxes for each section.

### TUI Framework Choice (blessed / blessed-contrib)
To build this interactive terminal UI, Nyx uses Node.js libraries that simplify drawing in the terminal. The primary choice is **blessed**, a popular library for building CLI interfaces, possibly extended by **blessed-contrib** for higher-level widgets. Blessed provides the core capabilities: rendering boxes, text, handling keyboard input, etc., while blessed-contrib provides ready-made dashboard widgets like charts, graphs, and a grid layout system​[npmjs.com](https://www.npmjs.com/package/blessed-contrib/v/0.0.5#:~:text=blessed). Blessed-contrib is well-suited since it is literally designed to “build dashboards (or any other application) using ascii/ansi art and javascript”​[npmjs.com](https://www.npmjs.com/package/blessed-contrib/v/0.0.5#:~:text=blessed), exactly our use case. It extends blessed with additional widgets (even using braille characters to draw graphics in terminal in some cases). By leveraging these, we avoid reinventing the wheel for things like making a line chart or table.

Nyx’s dashboard will incorporate the following from these libraries:

- **Screen & Boxes:** We create a `blessed.screen` which represents the full terminal window. This screen will capture input (so we can handle keys like Ctrl+C to exit, or switch focus between panels if needed). We then create two main `blessed.box` elements: one for left, one for right, positioned using percentage widths (e.g., left box width "60%", right box width "40%", both height "100%"). The left box can contain a log sub-component and an input line component. The right box will be further split. Using blessed-contrib’s `grid` utility, we can define sub-layouts easily. For example:
    
    js
    
    CopyEdit
    
    `const grid = new contrib.grid({rows: 12, cols: 12, screen: screen}); const leftPanel = grid.set(0, 0, 12, 7, blessed.box, { /* left panel options */ }); const rightPanel = grid.set(0, 7, 12, 5, blessed.box, { /* right panel options */ }); // inside rightPanel, further set sub-grids or directly add widgets.`
    
    This would allocate 7 columns to left, 5 to right (roughly a 58%/42% split). Within the right panel, we can either nest another grid or directly position widgets with relative heights. Suppose we do another grid for right panel: e.g. 12 rows (vertical slices) x 1 col (full width), then assign:
    
    - Tasks Tree widget spans rows 0-6,
        
    - Agent Status (maybe a table) spans rows 7-9,
        
    - Stats text spans rows 10-11, etc. This ensures a nicely structured layout that adapts to terminal size changes (blessed should recalc percentages).
        
- **Widgets:** We will use appropriate widgets for each section: the **Tree** widget from blessed-contrib for the task list (which supports expanding/collapsing and displays hierarchical data with lines​[lightyears1998.github.io](https://lightyears1998.github.io/blessed-docs/widgets/blessed-contrib/tree/#:~:text=,Default%20%3A%20true)), a **Table** or **List** for agent status, simple **Text** or **Paragraph** widgets for the stats (or possibly **LCD display** widget for fun, but numeric text is fine). If we incorporate a progress bar or chart, blessed-contrib has:
    - `contrib.gauge` for a horizontal gauge (could show overall progress or memory usage),
    - `contrib.donut` for a circular chart (could show tasks done vs pending),
    - `contrib.bar` for bar charts (maybe not needed here),
    - `contrib.line` for any time-series (not particularly needed unless we plot something like memory over time, which is overkill). For simplicity, the initial implementation might avoid too many fancy graphs and stick to textual representations, which are more straightforward and less error-prone in the first iteration. The framework allows adding those later without structural changes.
- **Interactivity:** The TUI is interactive. The user can scroll the left log (if it overflows) using keyboard (Page Up/Down or arrow keys), and similarly, potentially scroll or navigate the right panel (for example, the tree widget can be made focusable so the user can collapse/expand nodes or scroll through the task list if it’s long). Blessed automatically supports key events for many widgets. We will, for instance, allow the user to switch focus between left and right panels by pressing a key (maybe Tab or a specific function key). When the left panel (log) is focused, user input goes to the input box. When the right panel is focused, arrow keys could navigate the tree or table. This is an advanced usability enhancement; at minimum, the left panel should be default focused so the user can always type commands. The right panel is mostly read-only, but making it navigable (especially if content overflows) is useful.
- **Performance Considerations:** Updating the UI frequently should be done carefully. Blessed is relatively efficient, but we should avoid re-rendering the entire screen at an extremely high frequency. The design will use an **event-driven approach**: the underlying state will change at certain points (task start/finish, etc.), at which time we update the corresponding widget and call `screen.render()` to redraw. Each event might cause a full redraw, which is fine if events are at human-scale frequency (dozens per second at most; likely far less given tasks involve AI calls that take seconds). If needed, a small throttle or batch update can be done (e.g., if many tasks finish near-simultaneously, batch the update). Blessed’s drawing of static text and lines is fast, and our usage of blessed-contrib widgets (which often double-buffer or only update diffs) will mitigate flicker.
- **Alternate Libraries:** We considered alternatives like **Ink** (a React-based CLI UI library) which provide a declarative way to build TUIs with component state that auto-updates. Ink could manage state with hooks and re-render on state changes. However, integrating Ink with our event emitter might complicate things and Ink might not have all the nice widgets (charts) readily available. Blessed/blessed-contrib offers a proven solution specifically tailored for ASCII dashboards, and is more than capable for our needs​[npmjs.com](https://www.npmjs.com/package/blessed-contrib/v/0.0.5#:~:text=blessed). We stick with blessed for this implementation. (Another alternative was **terminal-kit** or **neo-blessed**, but blessed-contrib’s higher-level components tilt the choice in its favor.)

### Observable State & Event Emission
To achieve real-time updates in the dashboard, Nyx implements an **observable state store** that links the core logic to the UI. The idea is that whenever the internal state (tasks, agent status, etc.) changes, the UI should reflect it immediately without manual refresh from the user. We accomplish this by using Node.js’s event-driven capabilities and/or a reactive data store.

- **Event Emitters:** The simplest approach is to leverage Node’s built-in `EventEmitter` pattern. The Orchestrator, being central, can serve as the primary event emitter. As tasks and agent statuses change, it calls something like `this.events.emit('task_updated', data)` or `this.events.emit('agent_status', data)`. The dashboard module at startup would subscribe to these events: e.g., `orchestrator.events.on('task_updated', onTaskUpdate)`, providing callback functions that update the UI. Node’s event system allows multiple listeners, so the orchestrator can have one listener that maybe logs to console and another that updates UI, etc., if needed. This loosely couples the orchestrator to the dashboard – the orchestrator just emits events and doesn’t need to know who listens​[nodejs.org](https://nodejs.org/api/events.html#:~:text=Much%20of%20the%20Node,to%20be%20called). We ensure to define event names and data schemas clearly (for instance, `'task_updated'` might carry `{taskId, newStatus}`, `'task_progress'` could carry `{completedCount, totalCount}`, `'agent_status'` carries `{agentName, status, taskId}` etc.).
- **Reactive Store Alternative:** An alternative or complementary method is to maintain a single **observable state object** (containing all the info we want to display) and use a library or custom code to watch for changes. For example, we could have a global `dashboardState` object:
    
    js
    
    CopyEdit
    
    `dashboardState = { tasks: [...], agents: [...], stats: {...} };`
    
    When the orchestrator changes a value, it calls a setter that notifies the UI. This could be done with proxies (`Proxy` object to trap changes) or with a library like **MobX** (which makes plain object properties observable and triggers reactions on change). Another lightweight option is using **RxJS Subjects**: e.g., have an RxJS BehaviorSubject for the tasks list, and subscribe the UI to it. However, introducing RxJS might be overkill for a CLI tool. The event approach is straightforward and sufficient.
- **Integration Points:** The orchestrator will emit events at these points:
    - After the Planner finishes and provides the task list, emit `plan_ready` (with the task graph data).
    - Just before a task is started (and agent invoked), emit `task_started` (with task ID or object).
    - When a task is marked completed (successfully), emit `task_completed` (with task ID).
    - If a task fails, emit `task_failed` (with task ID and error info).
    - If the orchestrator adds a new task dynamically (some systems might adjust plans on the fly), emit `task_added`.
    - When an agent begins work on a task, emit `agent_status` (with agent name and task info, as agent now busy).
    - When an agent finishes (or if it goes idle), emit `agent_status` again (with updated status).
    - Periodically or on certain triggers, emit `stats_update` (with the latest counts for tasks done, API calls, etc.). We might not need a separate event if we can derive stats from task events, but having a periodic tick (say every few seconds) to update time elapsed and memory usage might be useful. This could be a simple setInterval that emits `stats_update` events.
    
    On the dashboard side, each of these events is handled:
    - `plan_ready`: initialize the Task Tree widget with the task list. We convert the task graph into the tree’s data format. Blessed-contrib’s tree expects a nested object structure with `children`. We can recursively build that from our task graph. Once done, call `tree.setData(taskTreeObject)` and maybe `screen.render()`.
    - `task_started`: find the task in the tree widget (if the widget allows direct access) and mark it, or simply re-set the data with an updated version that has that task’s status changed. If using a simple text list instead of a complex widget, we might regenerate the list of task lines and update a blessed.text element. But since blessed-contrib tree likely can be updated by mutating its data and calling setData again, that might be the way. Also update progress bar / counter.
    - `task_completed`/`task_failed`: similarly update statuses. Also update the counters in Stats (increment completed count, etc.). Possibly update an overall progress percentage and call progressBar.setProgress(pct).
    - `agent_status`: update the Agent table. For example, we maintain an array of [agent, status, currentTask] for the table. On event, we find the agent’s entry and update it, then call table.setData(newData).
    - `stats_update`: update the Stats text box content. For example, create a formatted string:
        
        yaml
        
        CopyEdit
        
        `Tasks done: 3/5   LLM calls: 7   Elapsed: 00:02:45   Memory: 150 MB`
        
        and do statsBox.setContent(thatString). Then render.
        
    Blessed (and blessed-contrib) require calling `screen.render()` to reflect changes. If we update a widget and call render, it will redraw. One could call render on every small change, or batch some changes then call once. We will likely call it at least at each event boundary. The Node event loop will intermix these with other logic, but since our events are mostly triggered synchronously after state changes, the UI might refresh quite fast. That’s okay.
- **Example Implementation Detail:** Suppose the Orchestrator marks task 2 as started. It does:
    `this.events.emit('task_started', { id: 2, name: "Fetch weather data" });`
    **In the dashboard setup code, we had:**
    ``orchestrator.events.on('task_started', ({id, name}) => {     // find node in tree data by id and update it, e.g.:     tasksTreeData.children[name].name = `[In Progress] ${name}`;     tasksTree.setData(tasksTreeData);     // also update "current task" label and increment running count if needed     currentTaskText.setContent(`Current Task: ${name}`);     screen.render(); });``
- **Ensuring Clean Integration:** The observability layer (events + dashboard) is designed to be **additive**. If the dashboard is disabled, the orchestrator’s event emits simply have no listeners (which is harmless), or we can even not emit if not needed. The event emission has negligible impact on performance and does not alter the decision-making flow, so the core logic is unchanged whether or not the dashboard is active. This is important for testing and headless use: we can run Nyx with the dashboard turned off and still have the same behavior in terms of planning and executing tasks. The separation via events ensures loose coupling. Essentially, the Orchestrator and Agents **broadcast** what’s happening, and the Dashboard **subscribes** to those broadcasts to display information – a classic observer pattern​[nodejs.org](https://nodejs.org/api/events.html#:~:text=Much%20of%20the%20Node,to%20be%20called). We could in the future have multiple observers (for example, a file logger that writes a log file with all events) without touching the core logic.
- **Reactive Libraries:** If we were to use a reactive library (like MobX or RxJS) for the state, the approach would be slightly different. We might have a MobX observable `dashboardState` and use MobX’s reactions to update the UI. Or with RxJS, we might have `taskSubject`, `agentSubject` etc., where an event triggers a `.next()` on those subjects, and the UI subscribed via `.subscribe()` updates accordingly. These approaches achieve the same result, but given the simplicity of our needs, we lean on EventEmitter which is built-in and well-understood. (In JavaScript terms, it’s effectively the pub-sub mechanism for our app state changes.)
### Dashboard Example Walk-through
To illustrate how everything comes together, consider a simple scenario: the user starts Nyx with a request. We show how the dashboard reflects the process step by step:
1. **User Input:** The user runs `nyx` in their terminal. Nyx starts, the screen is initialized. The left panel shows a prompt like `Nyx>` and the user types: _“Generate a simple Python script that prints numbers 1 to 5.”_ They hit Enter. This input goes to the Orchestrator (via the CLI input handler).
2. **Planning Phase:** The Orchestrator invokes the Planner agent with the user’s request. In the left log panel, Nyx echoes something like “User: Generate a simple Python script that prints numbers 1 to 5.” (Nyx might print the user command for clarity). It might then print “Orchestrator: Planning tasks...”. Meanwhile, nothing is in the right panel’s task list yet because planning isn’t done. Once the Planner returns a plan (say it comes up with two tasks: Task1 – “Write the Python script code”, Task2 – “Run the script to verify output”), the Orchestrator emits `plan_ready`. The dashboard’s right panel then displays those tasks in the Task Plan section: for example:
```mathematica 
Tasks: [ ] Write the Python script code   (Task 1) [ ] Run the script to verify output   (Task 2)

A progress bar maybe shows 0/2 tasks done (0%). The left panel log might show “Planner: Plan complete – 2 tasks identified.”.
```    
3. **Executing Task 1:** The Orchestrator moves to execute Task 1. It assigns the Coding Agent to this task. It emits `task_started` for Task 1, and `agent_status` for the Coding Agent now busy. The dashboard updates: Task 1 in the list might change to “[In Progress] Write the Python script code”. The Agent Status section updates to “CodingAgent: Running task ‘Write the Python script code’”. Stats might update “Current Task: Write the Python script code”. The left panel log shows details of what’s happening: e.g. “CodingAgent: Generating code for Task 1...”. After a couple of seconds, the Coding Agent finishes. It might emit (or Orchestrator emits on its behalf) `task_completed` for Task 1, and an `agent_status` update for CodingAgent now idle. The dashboard updates Task 1 to “[✔] Write the Python script code” (perhaps in green). Progress bar goes to 50%. Agent status says “CodingAgent: Idle” now. Also, Nyx likely wrote the code to a file, say `script.py`. The left panel log will mention “CodingAgent: Created file script.py”. That message also could appear in a “dynamic info” section on the right if we chose to display generated file names there, but currently we just log it.
4. **Executing Task 2:** Now Orchestrator moves to Task 2 (depends on Task 1 which is done). It assigns the Execution Agent. Emits `task_started` (Task 2) and `agent_status` (ExecutionAgent busy). Dashboard marks Task 2 “[In Progress] Run the script...”, and agent status “ExecutionAgent: Running task ‘Run the script…’”. The Execution Agent runs the file. Suppose it runs quickly and prints output “1 2 3 4 5”. That output might be captured and shown in the left panel log as well (or as part of Nyx logs: “ExecutionAgent: Script output: 1 2 3 4 5”). On success, ExecutionAgent reports back, Orchestrator emits `task_completed` (Task 2) and updates agent status. Dashboard marks Task 2 done, progress bar 100%, etc.
5. **Completion:** All tasks are done, Orchestrator might print “All tasks completed successfully.” in the left panel. The stats might show “Tasks: 2/2 completed, Failures: 0, LLM calls: 1” (assuming one call to OpenAI was made by the CodingAgent). The user sees the entire plan executed in both the log and the visual summary. The right panel now has the full plan with all tasks checked off, giving a nice summary of what was done. The user can scroll up in the left panel to see any detailed logs if needed. They can now type a new command at the prompt or exit.

This example shows how the two panels complement each other: left is verbose and scrollable (every detail, akin to a console log), right is structured and high-level (the plan and key metrics). This makes Nyx’s operation much easier to follow, especially for long or complex tasks with many steps.
## Extensibility and Code Organization
One of the goals for Nyx is to be easily extensible and maintainable. By organizing the code into cohesive modules, developers can add features or improve components without touching unrelated parts. Below is a suggested directory structure and notes on how the code is organized:

```
/src
├── cli/
│   └── nyx.ts             # Entry point for CLI interface, session loop
├── core/
│   ├── Orchestrator.ts    # Core Orchestrator class
│   ├── TaskGraph.ts       # Task and TaskGraph definitions, dependency handling
│   └── MemoryManager.ts   # (optional) Session memory handler
├── planner/
│   └── Planner.ts         # Planner class or functions (LLM prompt logic for planning)
├── agents/
│   ├── Agent.ts           # Base Agent interface/class
│   ├── CodeAgent.ts       # Implementation for code-writing tasks
│   ├── TestAgent.ts       # Implementation for test-writing tasks
│   ├── DebugAgent.ts      # Implementation for debugging/fixing tasks
│   └── ...                # Other agents (DocAgent, etc.)
├── tools/
│   ├── Tool.ts            # Base Tool interface/class
│   ├── FileTools.ts       # Implementations: readFile, writeFile, etc.
│   ├── ShellTool.ts       # Implementation for shell/command execution
│   ├── WebTool.ts         # Implementation for web search/fetch
│   └── ...                # Additional tools
├── llm/
│   ├── LLMClient.ts       # Interface to the LLM API (OpenAI, etc.)
│   ├── Prompts.ts         # Templates for various prompts (planner, code agent, etc.)
│   └── ContextUtil.ts     # Utilities for managing context windows (truncation, etc.)
├── utils/
│   ├── Logger.ts          # Logging utility
│   ├── Config.ts          # Configuration (API keys, settings)
│   └── Helper.ts          # Misc helpers
└── index.ts               # Possibly an entry to run Nyx outside CLI (for tests or integration)
```

In this structure:
- The **CLI** module is separate, so Nyx could potentially be used via other interfaces (e.g., a web UI or an API) by bypassing or reusing the Orchestrator directly.
- The **Core** directory holds orchestrator and related core logic like the TaskGraph and memory. This is the glue that ties planning, agents, and tools together.
- The **Planner** is encapsulated; you could swap out `Planner.ts` for a different implementation (say a rule-based planner or a different LLM prompt) and as long as it returns a proper TaskGraph, the orchestrator can work with it.
- The **Agents** each reside in their own module, making it easy to modify their behavior or add new ones. They can share code via the base class in `Agent.ts` (for example, common methods to call the LLM or handle tool actions).
- The **Tools** are each in their own module or grouped logically. For instance, FileTools.ts might export multiple related tool classes. All tools implement a common interface, and the orchestrator initializes them (for example, an array of Tool instances) and provides their descriptions to the Planner/Agents in prompts.
- The **LLM** folder contains the integration with the language model. By isolating it, one could switch from one provider to another, or add a local LLM, etc., by changing this module. The `Prompts.ts` might define constants or functions that generate the system prompts and few-shot examples for different contexts, which makes prompt tuning easier (one central place to edit the prompt for code generation, for example).
- **Utilities** include generic helpers like logging (so we can easily turn on/off debug logging of prompts and responses, which is very useful in development) and config (like reading an API key from env variables, or global settings such as “max parallel tasks”).

**Adding a New Tool – Example:** Suppose we want to add a **LintTool** that runs a linter (ESLint) on the code.
- We create `LintTool.ts` implementing `Tool`. It might run a shell command like `npx eslint .` and return the output.
- We add an instance of `LintTool` to the orchestrator’s tool registry.
- We update the prompt templates where we list available tools to the LLM. For example, in the system prompt we might have a section: “You can use the following tools when needed: 1) read_file - reads a file, 2) write_file - writes to a file, 3) run_tests - runs the test suite, 4) lint_code - run a linter on the code.” (And ensure the agent knows what each does.)
- Now the agent (especially a DebugAgent or final QA agent) could decide to call `lint_code` if it’s appropriate (maybe we would explicitly have a task for linting if user requested it or as a default quality step).
- No other part of the code needs to change; if the agent outputs an action calling the `lint_code` tool, the orchestrator will match it to the LintTool and execute it.

**Adding a New Agent – Example:** Suppose we want to add an **OptimizationAgent** that reviews the final code for performance improvements.
- We create `OptimizationAgent.ts` implementing Agent. Perhaps it will read all code files and use the LLM to suggest any refactors or optimizations.
- If this agent is to be invoked automatically, the Planner needs to generate a task for it (like “Optimize the code”). We might tweak the Planner prompt to include an optimization step at the end, or we could allow the orchestrator to append such a task under certain conditions.
- Or, the user could explicitly request optimization as a separate command, which would trigger planning including that agent’s task.
- Register the new agent in orchestrator so that if a task with type “optimize” comes, it uses OptimizationAgent.
- Now we can incorporate that without disturbing other agent logic.

**Maintainability:** By dividing responsibilities, developers can work on improving each module in isolation:
- If the quality of code generation is not good, one can focus on improving the prompt or logic in `CodeAgent.ts` (or even swap the LLM for that agent to a code-specialized model).
- If planning is suboptimal, adjust `Planner.ts` prompts or parsing logic.
- If performance is an issue (maybe too slow), one might introduce caching in Tools (e.g., if repeatedly reading the same file) or adjust concurrency settings in Orchestrator.

**Advanced Considerations:** Nyx’s design leaves room for future enhancements:
- Integration with a persistent database or memory to carry knowledge across sessions (not just within one run).
- A learning mechanism where Nyx could analyze afterwards which decisions led to a lot of fix cycles and perhaps adjust its prompts or approach next time (this would be more of an ML addition on top of the static logic).
- Security layers for running untrusted code in a sandbox (important if Nyx executes arbitrary code).
- A richer user interface (maybe turning the CLI into a web-based IDE where you see files being written in real-time).
## Conclusion
Nyx’s implementation, as described, provides a blueprint for an AI coding assistant that is both **powerful** and **organized**. By breaking the problem of autonomous coding into modular components – planning, executing via specialized agents, and using tools to interact with the environment – Nyx can handle complex, multi-step development tasks reliably. The dependency-aware task graph and orchestrator logic ensure tasks are done in the right order (or in parallel when possible) to efficiently reach the user’s goal. The system is **extensible**: new capabilities can be added with minimal changes to the existing code, thanks to clearly defined interfaces and separation of concerns. Moreover, Nyx is built with real-world use in mind: it manages context and memory to handle interactive sessions, uses file locks and careful scheduling to avoid conflicts, and can recover from errors by re-planning or self-correcting as needed, without constant user guidance.

With this architecture, a developer or team can proceed to implement Nyx from scratch in Node.js/TypeScript, confident that the design will accommodate advanced reasoning and autonomous execution features. The result will be a flexible AI agent that significantly accelerates software development tasks by combining the strengths of LLMs (planning, code generation, reasoning) with robust engineering practices (testing, iterative refinement, modular design). 

